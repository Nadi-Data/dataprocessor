{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import readtextfile\n",
    "import writeavrofile\n",
    "import writetextfile\n",
    "import writeparquetfile\n",
    "import utils\n",
    "import dask.dataframe as dd\n",
    "import pandas as pd\n",
    "import ray\n",
    "import time\n",
    "from timeit import default_timer as timer\n",
    "import io\n",
    "import uuid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "2021-01-12 21:52:54,063\tINFO worker.py:654 -- Connecting to existing Ray cluster at address: 192.168.29.242:6379\n"
     ]
    }
   ],
   "source": [
    "if ray.is_initialized:\n",
    "    ray.init(address=\"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Time taken : 0.034280551999998465 seconds for reading file '/Users/sriyan/Downloads/sales_50mb_1500000.csv'\n"
     ]
    }
   ],
   "source": [
    "df = readtextfile.ReadTextFile(ipfile='/Users/sriyan/Downloads/sales_50mb_1500000.csv',\n",
    "                                   ipschemafile='/Users/sriyan/Documents/dataprocessor/schema/sample_csv_file.schema',\n",
    "                                   delimiter=',', skiprows=1, parallel=4).read_using_dask()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\u001b[2m\u001b[36m(pid=21916)\u001b[0m duration = 9.02068488  seconds for transform\n",
      "\u001b[2m\u001b[36m(pid=21917)\u001b[0m duration = 9.098256571  seconds for transform\n",
      "\u001b[2m\u001b[36m(pid=21915)\u001b[0m duration = 9.081197728000001  seconds for transform\n",
      "\u001b[2m\u001b[36m(pid=21918)\u001b[0m duration = 9.173062334  seconds for transform\n",
      "\u001b[2m\u001b[36m(pid=21926)\u001b[0m duration = 0.31037634800000014  seconds for combining data for repartition\n",
      "\u001b[2m\u001b[36m(pid=21938)\u001b[0m duration = 0.5297652750000001  seconds for preparing partition based on keys\n",
      "\u001b[2m\u001b[36m(pid=21942)\u001b[0m duration = 0.519340717  seconds for preparing partition based on keys\n",
      "\u001b[2m\u001b[36m(pid=21941)\u001b[0m duration = 0.8600921370000001  seconds for preparing partition based on keys\n",
      "\u001b[2m\u001b[36m(pid=21951)\u001b[0m duration = 0.47566074999999985  seconds for preparing partition based on keys\n",
      "\u001b[2m\u001b[36m(pid=21957)\u001b[0m duration = 7.444226695  seconds for writing\n",
      "\u001b[2m\u001b[36m(pid=21953)\u001b[0m duration = 16.010869579  seconds for writing\n",
      "\u001b[2m\u001b[36m(pid=21948)\u001b[0m duration = 18.764007976000002  seconds for writing\n",
      "\u001b[2m\u001b[36m(pid=21954)\u001b[0m duration = 26.861731996  seconds for writing\n",
      "duration = 46.82158665399993  seconds\n",
      "parallelism= 4\n"
     ]
    }
   ],
   "source": [
    "@ray.remote\n",
    "class Test():\n",
    "\n",
    "    def transform(self, df, partition_no): \n",
    "        '''All transformations before repartition'''\n",
    "        start = timer()\n",
    "        \n",
    "        df = df.get_partition(partition_no).compute()\n",
    "        df['Order_Date'] = utils.Utils().parse_dates_using_lookup(df['Order_Date'])\n",
    "        df['Ship_Date'] = utils.Utils().parse_dates_using_lookup(df['Ship_Date'])\n",
    "        #df = df[df['Region'] == 'Europe']\n",
    "\n",
    "        print(\"duration =\", timer() - start, \" seconds for transform\")\n",
    "        return df\n",
    "\n",
    "    def concat_dfs(self, dflist):\n",
    "        start = timer()\n",
    "        '''Combine all dataframes to repartition them based on key'''\n",
    "        dfs = pd.concat(dflist)\n",
    "        #print(dfs.index)\n",
    "        print(\"duration =\", timer() - start, \" seconds for combining data for repartition\")\n",
    "        return dfs\n",
    "\n",
    "    def re_partition_sort_data(self, df, partition_metadata, partition_keys, sort_keys):\n",
    "        start = timer()\n",
    "        '''Prepare partitions based on partition metadata'''\n",
    "        df = df[df[partition_keys].isin(partition_metadata)]\n",
    "        if sort_keys is not None:\n",
    "            '''Sort data based on sort keys'''\n",
    "            df.sort_values(by=sort_keys)\n",
    "        print(\"duration =\", timer() - start, \" seconds for preparing partition based on keys\")\n",
    "        return df\n",
    "\n",
    "    def write_file(self, df, partition_no):\n",
    "        start = timer()\n",
    "        '''\n",
    "        df = dd.from_pandas(df, npartitions=1)\n",
    "        writeparquetfile.WriteParquetFile(ipdf=df, opfile=\"/tmp/data/sample_with_partitions\",\n",
    "                                      compression='snappy', engine='pyarrow', append=False, overwrite=True, write_metadata_file=False).write_using_dask()'''\n",
    "        filename = \"/tmp/data/sample_textfile.\" + str(partition_no) + \".txt\"\n",
    "        df.to_csv(filename, sep='|', header=None, encoding='utf-8')\n",
    "        #writeavrofile.WriteAvroFile(df, partition_no, '/Users/sriyan/Downloads/sample_avro_file').write_using_fastavro()\n",
    "        print(\"duration =\", timer() - start, \" seconds for writing\")\n",
    "        return 1\n",
    "\n",
    "\n",
    "start = timer()\n",
    "try:\n",
    "    actors = {}\n",
    "    result_ids_step1 = []\n",
    "    completed_dfs_step1 = []\n",
    "    completed_ids_step1 = []\n",
    "\n",
    "    result_ids_step2 = []\n",
    "    completed_ids_step2 = []\n",
    "\n",
    "    result_ids_step3 = []\n",
    "    completed_dfs_step3 = []\n",
    "    completed_ids_step3 = []\n",
    "\n",
    "    result_ids_step4 = []\n",
    "    completed_ids_step4 = []\n",
    "\n",
    "    npartitions = df.npartitions\n",
    "    \n",
    "    ################################STEP1##################################\n",
    "\n",
    "    '''Workers for executing step1 tasks'''\n",
    "    for i in range(npartitions):\n",
    "        actors[i] = Test.remote()\n",
    "        result_ids_step1.append(actors[i].transform.remote(df, i))\n",
    "    \n",
    "    '''Get step1 status'''\n",
    "    while len(result_ids_step1):\n",
    "        done_ids, result_ids_step1 = ray.wait(result_ids_step1)\n",
    "        completed_ids_step1.extend(done_ids if isinstance(done_ids, list) else [done_ids])\n",
    "    \n",
    "    '''Prepare list of data frames to be merged '''\n",
    "    dflist = [ray.get(x) for x in completed_ids_step1]\n",
    "\n",
    "    '''Close all step1 workers'''\n",
    "    for x in actors.keys():\n",
    "        ray.kill(actors[x])\n",
    "    \n",
    "    ################################STEP2##################################\n",
    "\n",
    "    '''workers for step2 tasks'''\n",
    "    actors[completed_ids_step1[0]] = Test.remote()\n",
    "    result_ids_step2.append(actors[completed_ids_step1[0]].concat_dfs.remote(dflist))\n",
    "    \n",
    "    '''Get step2 status'''\n",
    "    while len(result_ids_step2):\n",
    "        done_ids, result_ids_step2 = ray.wait(result_ids_step2)\n",
    "        completed_ids_step2.extend(done_ids if isinstance(done_ids, list) else [done_ids])\n",
    "    \n",
    "    #print(completed_ids_step2)\n",
    "\n",
    "    '''Get the merged data frame'''\n",
    "    df_concat = [ray.get(x) for x in completed_ids_step2][0]\n",
    "    #print(df_concat.info)\n",
    "\n",
    "    '''Close all step2 workers'''\n",
    "    for x in actors.keys():\n",
    "        ray.kill(actors[x])\n",
    "    \n",
    "    ################################STEP3##################################\n",
    "\n",
    "    '''Get the partitions metadata based on keys'''\n",
    "    partition_keys = 'Region'\n",
    "    sort_keys = ['Order_Date']\n",
    "    partition_metadata = utils.Utils().define_partitions(seq = list(df_concat[partition_keys].value_counts(dropna=False).keys()), \n",
    "                                                         num_of_partitions = npartitions)\n",
    "\n",
    "    '''Repartition data: partition using metadata >> sort >> send to workers'''\n",
    "    for i, x in enumerate(partition_metadata):\n",
    "        actors[i] = Test.remote()\n",
    "        result_ids_step3.append(actors[i].re_partition_sort_data.remote(df_concat, x, partition_keys, sort_keys))\n",
    "    \n",
    "    '''Get step3 status'''\n",
    "    while len(result_ids_step3):\n",
    "        done_ids, result_ids_step3 = ray.wait(result_ids_step3)\n",
    "        for done_id in done_ids:\n",
    "            actors[done_id] = Test.remote()\n",
    "            result_ids_step4.append(actors[done_id].write_file.remote(ray.get(done_id), uuid.uuid4()))\n",
    "        completed_ids_step3.extend(done_ids if isinstance(done_ids, list) else [done_ids])\n",
    "    \n",
    "    ################################STEP4##################################\n",
    "\n",
    "    '''Get step4 status'''   \n",
    "    while len(result_ids_step4):\n",
    "        done_ids_step4, result_ids_step4 = ray.wait(result_ids_step4)\n",
    "        completed_ids_step4.extend(done_ids_step4 if isinstance(done_ids_step4, list) else [done_ids_step4])\n",
    "finally:\n",
    "    ''' All temporary files should be removed\n",
    "    ray.get(completed_ids_step1)\n",
    "    ray.get(completed_ids_step2)\n",
    "    ray.get(completed_ids_step3)\n",
    "    ray.get(completed_ids_step4)'''\n",
    "    for x in actors.keys():\n",
    "        ray.kill(actors[x])\n",
    "print(\"duration =\", timer() - start, \" seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}